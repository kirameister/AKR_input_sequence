# データ収集

配列を作成するにあたり作成者の「カン」というのは大事だが (というか、最終的にはカン頼みになってしまうのが配列作成の現状だと思う)、やみくもに配列を決めても効果的な配列が生まれる可能性は低いと思う。やはりカンに頼る前に、なんらかの仮説のようなものを立てるためのデータは必要だろう。新下駄配列の作成時にも行われたことだが、私も N-gram のデータが重要になってくると考える。

ここで必要なデータというのは、以下のような特性を持ったものだ:

* 色々な分野から得られた、ある程度バランスの取れたデータである
* 読みの情報が入っているデータである (必ずしも必要ではないが、その場合私の手で mecab などを介して読み情報を作成する必要がある)
* できれば人間の手で作られた、または人間のチェックが入ったデータである
    * これは mecab などの読み情報の出力で一部不完全と思えるような箇所を見つけたため -- 実は公開されている新下駄配列作成のために使われたデータにもそのような (誤った読み方と思えるような内容を見つけている)
* ある程度のデータ量がある -- 新下駄配列では百万という数値があったが、私もこれぐらいが最低限必要だろうと考える

少し調べてみたのだが、以下にある京都大学のページからダウンロード可能な BCCWJ(コアデータ) をベースにした仮名漢字変換用の N-gram を使うことにする。

http://www.lsta.media.kyoto-u.ac.jp/member/gologo/lm.html

理由は以下の通り:

* 読み情報が与えられている
* 元々 BCCWJ のデータは (B が Balance を表しているように) 色々な分野のテキストデータから作成されたデータである
* BCCWJ のコアデータは人力でアノテーションが行われたらしい -- この人力入力が読み情報に対しても行われたかは論文からは明らかではないが、meacb などで "二十日/にじゅうにち" となるところ、このデータではちゃんと "二十日/はつか" と表記されている
* 文字レベルではなく単語レベルで tokenization は行われているが、これは私の後処理で対処できそう
* 単語数だけで 100 万トークンある (文字ベースだと更にある)

ファイルをダウンロードして、`bz2` ファイルを `bunzip2` にて解凍した後、以下のようにして utf-8 に変換する:

```
iconv 1-gram.fwk -f EUC-JP -t UTF8 -o utf-1gram.txt
```

上にも書いた通り、ここで得られるファイルの中身は単語ベースの N-gram である。これを文字ベースに変換する必要がある。


